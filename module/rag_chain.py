import os
import json
import logging
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

from langchain.chat_models import init_chat_model
from langchain.embeddings import HuggingFaceEmbeddings

from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

from langchain.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate
from langchain_pinecone import PineconeVectorStore

from module.sql_utils import is_hr_team

load_dotenv()
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------
# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
# ----------------------------
def get_prompt(user_department):
    logging.info("ğŸ§± í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")

    base_template = """
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì•„ë˜ì˜ ê·œì¹™ì„ ë”°ë¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ê³ , í•´ë‹¹ ì¿¼ë¦¬ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ê°€ì¥ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì‹œê°í™” ì°¨íŠ¸ë¥¼ Vega-Lite í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

    # ê·œì¹™:
    1. ëª¨ë“  íŒ€ì€ temp_datasetì— í¬í•¨ëœ í…Œì´ë¸”(card_members, card_sales, card_credit ë“±)ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. ë‹¨, hr_dataset ë‚´ í…Œì´ë¸”(position, salary ë“±)ì€ ì¸ì‚¬íŒ€ë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. {hr_rule}
    3. ì§ˆë¬¸ìëŠ” ì¼ìƒì–´ë¡œ ì§ˆë¬¸í•  ìˆ˜ ìˆìœ¼ë©°, ì´ì— ë§ëŠ” ì ì ˆí•œ SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    4. ê°€ëŠ¥í•œ ê²½ìš° JOIN í‚¤(ì˜ˆ: member_no)ë¥¼ í™œìš©í•˜ì—¬ í•„ìš”í•œ í…Œì´ë¸”ì„ ì—°ê²°í•˜ì„¸ìš”.
    5. temp_datasetì˜ ê° í…Œì´ë¸”ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ context_schemaì— ì œê³µë©ë‹ˆë‹¤.
    6. SQLì€ BigQuery ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    7. ì‹œê°í™”ëŠ” í‘œì¶œí•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼, ë°ì´í„°ì˜ ì§‘ê³„ ìˆ˜ì¤€ ë“±ì„ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì°¨íŠ¸ë¥¼ ì„ ì •í•˜ì„¸ìš” (ì˜ˆ: ë§‰ëŒ€, ì„ , íŒŒì´, êº¾ì€ì„ , ëˆ„ì  ë“±).
    8. ì¶”í›„ ì´ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ë¥¼ ì‹œê°í™”í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•  ì˜ˆì •ì´ë¯€ë¡œ, ì°¨íŠ¸ JSONì€ ë°˜ë“œì‹œ ë°ì´í„° êµ¬ì¡°ë¥¼ ëª…í™•íˆ í‘œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.

    # ì…ë ¥ ì •ë³´
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {question}

    [ëŒ€í™” ë‚´ì—­]
    {chat_history}

    [í…Œì´ë¸” ì„¤ëª…]
    {context_schema}

    # ì¶œë ¥ í˜•ì‹
    ```sql
    -- ê°„ë‹¨í•œ ì„¤ëª…
    SELECT ...
    FROM ...
    WHERE ...
    ```

    ```json
    // Vega-Lite í¬ë§·
    {
    ...
    }
    ```
    """

    hr_rule = "ì§ˆë¬¸ìê°€ ì¸ì‚¬íŒ€ì´ ì•„ë‹ˆë©´ hr_datasetì— ìˆëŠ” í…Œì´ë¸”ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”." if not is_hr_team(user_department) else "(ì§ˆë¬¸ìê°€ ì¸ì‚¬íŒ€ì´ë¯€ë¡œ hr_dataset ì‚¬ìš© ê°€ëŠ¥)"
    template = base_template.format(hr_rule=hr_rule)

    prompt_template = PromptTemplate(
        input_variables=["context_schema", "question", "chat_history"],
        template=template
    )

    human_prompt = HumanMessagePromptTemplate(prompt=prompt_template)
    final_prompt = ChatPromptTemplate(messages=[human_prompt])

    return final_prompt

def get_prompt_for_insight(request):
    prompt = f"""
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì´ë©°, ë¹„ì „ë¬¸ê°€ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‹œê°í™” ì°¨íŠ¸ë¥¼ í•´ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {request.question}

    [ì°¨íŠ¸ ì‹œê°í™” ìŠ¤í™ - Vega-Lite JSON]
    {json.dumps(request.chart_spec, indent=2, ensure_ascii=False)}

    [ì‚¬ìš©ì ì†Œì† ë¶€ì„œ]
    {request.user_department or "(ì•Œ ìˆ˜ ì—†ìŒ)"}

    [ëŒ€í™” ë§¥ë½]
    {request.chat_history or "(ì—†ìŒ)"}

    [ë°ì´í„° ì „ì²´]
    {json.dumps(request.data, indent=2, ensure_ascii=False)}

    ìœ„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒì„ ì‘ì„±í•˜ì„¸ìš”:
    1. ì–´ë–¤ ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ” ì°¨íŠ¸ì¸ì§€ ê°„ë‹¨í•œ ì„¤ëª…
    2. ê°€ì¥ ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ (ìˆ˜ì¹˜ ë¹„êµ, íŠ¸ë Œë“œ ë“±)
    3. ì‚¬ìš©ìê°€ ì´ ë°ì´í„°ë¥¼ ë³´ê³  ì–´ë–¤ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆëŠ”ì§€ ì œì•ˆ (ì‚¬ìš©ì ì†Œì† ë¶€ì„œ ë° ì‚¬ìš©ì ì§ˆë¬¸ ì°¸ê³ )

    ì´ ì„¤ëª…ì€ ë‰´ìŠ¤ë ˆí„°ë‚˜ ë¦¬í¬íŠ¸ì— ê·¸ëŒ€ë¡œ ì“¸ ìˆ˜ ìˆë„ë¡ í¬ë©€í•˜ê³  ëª…í™•í•˜ê²Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    """

    return prompt

# ----------------------------
# RAG ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
# ----------------------------

def set_rag_chain(question, user_department, pc):
    logging.info("ğŸ“¥ RAG ì²´ì¸ êµ¬ì„± ì‹œì‘")

    # Pinecone + Embedding
    logging.info("ğŸ”— Pinecone VectorStore ì´ˆê¸°í™” ì¤‘...")
    embedding = HuggingFaceEmbeddings(
        model_name="nlpai-lab/KURE-v1",
        model_kwargs={"device": "cpu"},  # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ "cuda"
        encode_kwargs={"normalize_embeddings": True}
    )

    schema_vectorstore = PineconeVectorStore(
        index=pc.Index("schema-index"),
        embedding=embedding
    )

    schema_retriever = schema_vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={"k": 3}
    )

    model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
    compressor = CrossEncoderReranker(model=model, top_n=3)

    schema_retriever_compression = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=schema_retriever
    )

    # Gemini LLM
    logging.info("ğŸ¤– Gemini LLM ì´ˆê¸°í™” ì¤‘...")
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    GEMINI_API_BASE = os.environ.get("GEMINI_API_BASE")
    
    # llm = ChatGoogleGenerativeAI(
    #     model="gemini-2.0-flash-lite",
    #     google_api_key=GEMINI_API_KEY,
    #     streaming=True,
    #     callbacks=[StreamingStdOutCallbackHandler()]
    # )

    llm = init_chat_model("gemini-2.0-flash-lite", model_provider="google_genai")

    # ì²´ì¸ ì •ì˜
    rag_chain = (
        {
            "chat_history": RunnableLambda(lambda x: user_department),
            "context_schema": schema_retriever_compression,
            "question": RunnablePassthrough()
        }
        | get_prompt(user_department)
        | llm
        | StrOutputParser()
    )

    answer = rag_chain.invoke(question)

    print("\n[ì°¸ê³ ëœ ë¬¸ì„œ ì¶œì²˜]:")
    for doc in rag_chain.last_run["source_documents"]:
        print("-", doc.metadata.get("source"))

    return answer

