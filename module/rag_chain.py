import os
import logging
from dotenv import load_dotenv

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

from langchain.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate
from langchain_pinecone import PineconeVectorStore

from utils.sql_utils import is_hr_team

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
load_dotenv()


class InsightRequest(BaseModel):
    question: str
    chart_spec: dict
    data: list  # list of dicts (DataFrame to_dict(orient="records"))
    chat_history: str | None = None
    user_department: str | None = None

# ----------------------------
# í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
# ----------------------------
def get_prompt(user_department):
    logging.info("ğŸ§± í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")

    base_template = """
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    ì•„ë˜ì˜ ê·œì¹™ì„ ë”°ë¼ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ê³ , í•´ë‹¹ ì¿¼ë¦¬ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ë°ì´í„°ë¥¼ ê°€ì¥ íš¨ê³¼ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ì‹œê°í™” ì°¨íŠ¸ë¥¼ Vega-Lite í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

    # ê·œì¹™:
    1. ëª¨ë“  íŒ€ì€ temp_datasetì— í¬í•¨ëœ í…Œì´ë¸”(card_members, card_sales, card_credit ë“±)ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    2. ë‹¨, hr_dataset ë‚´ í…Œì´ë¸”(position, salary ë“±)ì€ ì¸ì‚¬íŒ€ë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. {hr_rule}
    3. ì§ˆë¬¸ìëŠ” ì¼ìƒì–´ë¡œ ì§ˆë¬¸í•  ìˆ˜ ìˆìœ¼ë©°, ì´ì— ë§ëŠ” ì ì ˆí•œ SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    4. ê°€ëŠ¥í•œ ê²½ìš° JOIN í‚¤(ì˜ˆ: member_no)ë¥¼ í™œìš©í•˜ì—¬ í•„ìš”í•œ í…Œì´ë¸”ì„ ì—°ê²°í•˜ì„¸ìš”.
    5. temp_datasetì˜ ê° í…Œì´ë¸”ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ context_schemaì— ì œê³µë©ë‹ˆë‹¤.
    6. SQLì€ BigQuery ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    7. ì‹œê°í™”ëŠ” í‘œì¶œí•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼, ë°ì´í„°ì˜ ì§‘ê³„ ìˆ˜ì¤€ ë“±ì„ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ì°¨íŠ¸ë¥¼ ì„ ì •í•˜ì„¸ìš” (ì˜ˆ: ë§‰ëŒ€, ì„ , íŒŒì´, êº¾ì€ì„ , ëˆ„ì  ë“±).
    8. ì¶”í›„ ì´ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¨íŠ¸ë¥¼ ì‹œê°í™”í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•  ì˜ˆì •ì´ë¯€ë¡œ, ì°¨íŠ¸ JSONì€ ë°˜ë“œì‹œ ë°ì´í„° êµ¬ì¡°ë¥¼ ëª…í™•íˆ í‘œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.

    # ì…ë ¥ ì •ë³´
    [ì‚¬ìš©ì ì§ˆë¬¸]
    {question}

    [ëŒ€í™” ë‚´ì—­]
    {chat_history}

    [í…Œì´ë¸” ì„¤ëª…]
    {context_schema}

    # ì¶œë ¥ í˜•ì‹
    ```sql
    -- ê°„ë‹¨í•œ ì„¤ëª…
    SELECT ...
    FROM ...
    WHERE ...
    ```

    ```json
    // Vega-Lite í¬ë§·
    {
    ...
    }
    ```
    """

    hr_rule = "ì§ˆë¬¸ìê°€ ì¸ì‚¬íŒ€ì´ ì•„ë‹ˆë©´ hr_datasetì— ìˆëŠ” í…Œì´ë¸”ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”." if not is_hr_team(user_department) else "(ì§ˆë¬¸ìê°€ ì¸ì‚¬íŒ€ì´ë¯€ë¡œ hr_dataset ì‚¬ìš© ê°€ëŠ¥)"
    template = base_template.format(hr_rule=hr_rule)

    prompt_template = PromptTemplate(
        input_variables=["context_schema", "question", "chat_history"],
        template=template
    )

    human_prompt = HumanMessagePromptTemplate(prompt=prompt_template)
    final_prompt = ChatPromptTemplate(messages=[human_prompt])

    return final_prompt

def get_prompt_for_insight(request: InsightRequest):
    prompt = f"""
    ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ê°€ì´ë©°, ë¹„ì „ë¬¸ê°€ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ì‹œê°í™” ì°¨íŠ¸ë¥¼ í•´ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

    [ì‚¬ìš©ì ì§ˆë¬¸]
    {request.question}

    [ì°¨íŠ¸ ì‹œê°í™” ìŠ¤í™ - Vega-Lite JSON]
    {json.dumps(request.chart_spec, indent=2, ensure_ascii=False)}

    [ì‚¬ìš©ì ì†Œì† ë¶€ì„œ]
    {request.user_department or "(ì•Œ ìˆ˜ ì—†ìŒ)"}

    [ëŒ€í™” ë§¥ë½]
    {request.chat_history or "(ì—†ìŒ)"}

    [ë°ì´í„° ì „ì²´]
    {json.dumps(request.data, indent=2, ensure_ascii=False)}

    ìœ„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒì„ ì‘ì„±í•˜ì„¸ìš”:
    1. ì–´ë–¤ ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ” ì°¨íŠ¸ì¸ì§€ ê°„ë‹¨í•œ ì„¤ëª…
    2. ê°€ì¥ ì£¼ëª©í•  ë§Œí•œ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ (ìˆ˜ì¹˜ ë¹„êµ, íŠ¸ë Œë“œ ë“±)
    3. ì‚¬ìš©ìê°€ ì´ ë°ì´í„°ë¥¼ ë³´ê³  ì–´ë–¤ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆëŠ”ì§€ ì œì•ˆ (ì‚¬ìš©ì ì†Œì† ë¶€ì„œ ë° ì‚¬ìš©ì ì§ˆë¬¸ ì°¸ê³ )

    ì´ ì„¤ëª…ì€ ë‰´ìŠ¤ë ˆí„°ë‚˜ ë¦¬í¬íŠ¸ì— ê·¸ëŒ€ë¡œ ì“¸ ìˆ˜ ìˆë„ë¡ í¬ë©€í•˜ê³  ëª…í™•í•˜ê²Œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    """

    return prompt

# ----------------------------
# RAG ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
# ----------------------------
def set_rag_chain(question, user_department, pc):
    logging.info("ğŸ“¥ RAG ì²´ì¸ êµ¬ì„± ì‹œì‘")

    # Pinecone + Embedding
    logging.info("ğŸ”— Pinecone VectorStore ì´ˆê¸°í™” ì¤‘...")
    embedding = ()  # ì‹¤ì œ ì„ë² ë”© ê°ì²´ë¡œ êµì²´

    schema_vectorstore = PineconeVectorStore(
        index=pc.Index("schema-index"),
        embedding=embedding
    )

    schema_retriever = schema_vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={"k": 3}
    )

    model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
    compressor = CrossEncoderReranker(model=model, top_n=3)

    schema_retriever_compression = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=schema_retriever
    )

    # Gemini LLM
    logging.info("ğŸ¤– Gemini LLM ì´ˆê¸°í™” ì¤‘...")
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-pro",
        google_api_key=GEMINI_API_KEY
    )

    # ì²´ì¸ ì •ì˜
    rag_chain = (
        {
            "chat_history": RunnableLambda(lambda x: user_department),
            "context_schema": schema_retriever_compression,
            "question": RunnablePassthrough()
        }
        | get_prompt(user_department)
        | llm
        | StrOutputParser()
    )

    return rag_chain.invoke(question)


# import os
# import logging
# from dotenv import load_dotenv

# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_core.runnables import RunnablePassthrough, RunnableLambda
# from langchain_core.output_parsers import StrOutputParser
# from langchain.retrievers import ContextualCompressionRetriever
# from langchain.retrievers.document_compressors import CrossEncoderReranker
# from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# from langchain.prompts import ChatPromptTemplate, PromptTemplate, HumanMessagePromptTemplate
# from langchain_pinecone import PineconeVectorStore

# logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
# load_dotenv()


# # ----------------------------
# # ì¸ì‚¬íŒ€ ì—¬ë¶€ íŒë³„ í•¨ìˆ˜
# # ----------------------------
# def is_hr_team(department: str) -> bool:
#     return department.strip() == "ì¸ì‚¬íŒ€"


# # ----------------------------
# # í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ (íŒ€ì— ë”°ë¼ ë‚´ìš© ì¡°ì •)
# # ----------------------------
# def get_prompt(user_department):
#     logging.info("ğŸ§± í”„ë¡¬í”„íŠ¸ ìƒì„± ì¤‘...")

#     base_template = """
# ë‹¹ì‹ ì€ ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ê·œì¹™ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

# # ê·œì¹™:

# 1. ëª¨ë“  íŒ€ì€ temp_datasetì— í¬í•¨ëœ í…Œì´ë¸”(card_members, card_sales, card_credit ë“±)ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# 2. ë‹¨, hr_dataset ë‚´ í…Œì´ë¸”(position, salary ë“±)ì€ ì¸ì‚¬íŒ€ë§Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. {hr_rule}
# 3. ì§ˆë¬¸ìê°€ ì œê³µí•œ ì§ˆë¬¸ì€ ì¼ìƒì–´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ì§ˆë¬¸ì„ ì´í•´í•˜ê³ , ê´€ë ¨ í…Œì´ë¸”/ì»¬ëŸ¼ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ SQLì„ ì‘ì„±í•˜ì„¸ìš”.
# 4. ê°€ëŠ¥í•œ ê²½ìš° JOIN í‚¤(ì˜ˆ: member_no)ë¥¼ í™œìš©í•˜ì—¬ í•„ìš”í•œ í…Œì´ë¸”ì„ ì—°ê²°í•˜ì„¸ìš”.
# 5. temp_datasetì˜ ê° í…Œì´ë¸”ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ë˜ context_schemaì— ì œê³µë©ë‹ˆë‹¤.
# 6. SQLì€ BigQuery ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ë‹¨, ì§ˆë¬¸ìê°€ ê²°ê³¼ë¥¼ ì›í•˜ë©´ SELECT ë¬¸ì— í•„ìš”í•œ ì»¬ëŸ¼ì„ ëª…ì‹œí•´ ì£¼ì„¸ìš”.

# # ì…ë ¥ ì •ë³´

# [ì‚¬ìš©ì ì§ˆë¬¸]
# {question}

# [ì ‘ê·¼ ê°€ëŠ¥í•œ íŒ€]
# {chat_history}

# [í…Œì´ë¸” ì„¤ëª…]
# {context_schema}

# # ì¶œë ¥ í˜•ì‹:
# ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”.

# ```sql
# -- ê°„ë‹¨í•œ ì„¤ëª…: (ì´ ì¿¼ë¦¬ê°€ ë¬´ì—‡ì„ í•˜ëŠ”ì§€ í•œ ì¤„ ìš”ì•½)
# SELECT ...
# FROM ...
# WHERE ...
# ```

# í•„ìš” ì‹œ ORDER BY, LIMIT ë“±ì„ ì ì ˆíˆ ì¶”ê°€í•˜ì„¸ìš”.
# """

#     hr_rule = "ì§ˆë¬¸ìê°€ ì¸ì‚¬íŒ€ì´ ì•„ë‹ˆë©´ hr_datasetì— ìˆëŠ” í…Œì´ë¸”ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”." if not is_hr_team(user_department) else "(ì§ˆë¬¸ìê°€ ì¸ì‚¬íŒ€ì´ë¯€ë¡œ hr_dataset ì‚¬ìš© ê°€ëŠ¥)"
#     template = base_template.format(hr_rule=hr_rule)

#     prompt_template = PromptTemplate(
#         input_variables=["context_schema", "question", "chat_history"],
#         template=template
#     )

#     human_prompt = HumanMessagePromptTemplate(prompt=prompt_template)
#     final_prompt = ChatPromptTemplate(messages=[human_prompt])

#     return final_prompt


# # ----------------------------
# # RAG ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
# # ----------------------------
# def set_rag_chain(question, user_department, pc):
#     logging.info("ğŸ“¥ RAG ì²´ì¸ êµ¬ì„± ì‹œì‘")

#     # 1. VectorStore ì„¤ì •
#     logging.info("ğŸ”— Pinecone VectorStore ì´ˆê¸°í™” ì¤‘...")
#     embedding = ()  # ì—¬ê¸°ì— ì‹¤ì œ ì„ë² ë”© ê°ì²´ë¥¼ ë„£ìœ¼ì„¸ìš”

#     schema_vectorstore = PineconeVectorStore(
#         index=pc.Index("schema-index"),
#         embedding=embedding
#     )

#     # 2. Retriever ì„¤ì •
#     logging.info("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™” ì¤‘ (MMR + ReRanker)")
#     schema_retriever = schema_vectorstore.as_retriever(
#         search_type='mmr',
#         search_kwargs={"k": 3}
#     )

#     model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
#     compressor = CrossEncoderReranker(model=model, top_n=3)

#     schema_retriever_compression = ContextualCompressionRetriever(
#         base_compressor=compressor,
#         base_retriever=schema_retriever
#     )

#     # 3. Gemini LLM ì„¤ì •
#     logging.info("ğŸ¤– Gemini LLM ì´ˆê¸°í™” ì¤‘...")
#     GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
#     llm = ChatGoogleGenerativeAI(
#         model="gemini-1.5-pro",
#         google_api_key=GEMINI_API_KEY
#     )

#     # 4. ì²´ì¸ ì—°ê²°
#     logging.info("ğŸ”— RAG ì²´ì¸ êµ¬ì„± ì™„ë£Œ, ì§ˆì˜ ìˆ˜í–‰ ì¤‘...")
#     rag_chain = (
#         {
#             "chat_history": RunnableLambda(lambda x: user_department),
#             "context_schema": schema_retriever_compression,
#             "question": RunnablePassthrough()
#         }
#         | get_prompt(user_department)
#         | llm
#         | StrOutputParser()
#     )

#     answer = rag_chain.invoke(question)
#     logging.info("âœ… ì§ˆì˜ ì²˜ë¦¬ ì™„ë£Œ")

#     return answer
