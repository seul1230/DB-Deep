# 1. í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”
# pip install langchain pinecone-client transformers sentence-transformers openai

import os
from dotenv import load_dotenv

from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Pinecone
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import OpenAI  # ë˜ëŠ” HuggingFaceHub ë“± ëŒ€ì²´ ê°€ëŠ¥
# from pinecone import Pinecone, ServerlessSpec
import pinecone

import logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

load_dotenv()

logging.info("ğŸ“¦ ë¬¸ì„œ ì„ë² ë”© ë° Pinecone ì—…ë¡œë“œ ì‹œì‘")

# ----------------------------
# 2. Pinecone ì´ˆê¸°í™”
# ----------------------------

PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY")
if not PINECONE_API_KEY:
    raise ValueError("âŒ PINECONE_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤")
pinecone.init(
    api_key=PINECONE_API_KEY,      
    # environment="YOUR_PINECONE_ENV"      # "us-west4-gcp"
)
index_name = "schema-index"             # ì¸ë±ìŠ¤ ì´ë¦„

logging.info("ğŸ“¦ Pinecone ì´ˆê¸°í™”")

# ----------------------------
# 3. ë¬¸ì„œ ë¡œë“œ ë° ì¶œì²˜ ë©”íƒ€ë°ì´í„° ì¶”ê°€
# ----------------------------
text_files = ["card_members.txt", "card_credit.txt", "card_sales.txt"]  # ë‹¤ì¤‘ ë¬¸ì„œ ëª©ë¡
docs = []

for file_path in text_files:
    logging.info(f"ğŸ“„ ë¬¸ì„œ ë¡œë“œ ì¤‘: {file_path}")
    loader = TextLoader(file_path, encoding="utf-8")
    loaded_docs = loader.load()
    for doc in loaded_docs:
        doc.metadata["source"] = os.path.basename(file_path)
    docs.extend(loaded_docs)

# ----------------------------
# 4. ë¬¸ì„œ ë¶„í•  (chunk ë‹¨ìœ„)
# ----------------------------
logging.info("âœ‚ï¸ ë¬¸ì„œ chunk ë¶„í•  ì¤‘...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
splits = text_splitter.split_documents(docs)

# ----------------------------
# 5. KURE-v1 ì„ë² ë”© (Hugging Face ëª¨ë¸ ì‚¬ìš©)
# ----------------------------
logging.info("ğŸ” KURE ì„ë² ë”© ìƒì„± ì¤‘...")
embedding = HuggingFaceEmbeddings(
    model_name="nlpai-lab/KURE-v1",
    model_kwargs={"device": "cpu"},  # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ "cuda"
    encode_kwargs={"normalize_embeddings": True}
)

# ----------------------------
# 6. Pineconeì— ì—…ë¡œë“œ
# ----------------------------
logging.info("ğŸ“¡ Pineconeì— ë²¡í„° ì—…ë¡œë“œ ì¤‘...")
vectorstore = Pinecone.from_documents(
    documents=splits,
    embedding=embedding,
    index_name=index_name
)

logging.info("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ: ë¬¸ì„œ ì„ë² ë”© â†’ Pinecone ì—…ë¡œë“œ ì™„ë£Œ")


# # ----------------------------
# # 7. ì§ˆì˜ â†’ ë¬¸ì„œ ê²€ìƒ‰ â†’ ë‹µë³€ ìƒì„±
# # ----------------------------
# retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# qa_chain = RetrievalQA.from_chain_type(
#     llm=OpenAI(temperature=0),  # ğŸ”‘ OpenAI API í‚¤ê°€ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•¨
#     retriever=retriever,
#     return_source_documents=True
# )

# # ----------------------------
# # 8. ì˜ˆì‹œ ì§ˆì˜ ì‹¤í–‰
# # ----------------------------
# query = "VIP ê³ ê° ì¤‘ ìµœê·¼ 3ê°œì›”ê°„ ì¹´ë“œë¡ ì„ ë§ì´ ì“´ ì‚¬ëŒì€?"
# result = qa_chain.run(query)

# # ----------------------------
# # 9. ê²°ê³¼ ì¶œë ¥
# # ----------------------------
# print("\n[ë‹µë³€]:")
# print(result)

# print("\n[ì°¸ê³ ëœ ë¬¸ì„œ ì¶œì²˜]:")
# for doc in qa_chain.last_run["source_documents"]:
#     print("-", doc.metadata.get("source"))
