import os
import json
import logging
from dotenv import load_dotenv

from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_huggingface import HuggingFaceEmbeddings

from pinecone import Pinecone, ServerlessSpec
from langchain_pinecone import PineconeVectorStore

from llm.gemini import GeminiStreamingViaGMS, GeminiSyncViaGMS
from pipeline.propmt_templates import get_prompt, get_prompt_for_insight

load_dotenv()
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------
# RAG ì²´ì¸ êµ¬ì„± í•¨ìˆ˜
# ----------------------------

def set_rag_chain(question, user_department, schema_vectorstore):
    logging.info("ğŸ“¥ RAG ì²´ì¸ êµ¬ì„± ì‹œì‘")

    # Pinecone + Embedding
    logging.info("ğŸ”— Pinecone VectorStore ì´ˆê¸°í™” ì¤‘...")
    embedding = HuggingFaceEmbeddings(
        model_name="nlpai-lab/KURE-v1",
        model_kwargs={"device": "cpu"},  # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ "cuda"
        encode_kwargs={"normalize_embeddings": True}
    )

    schema_retriever = schema_vectorstore.as_retriever(
        search_type='mmr',
        search_kwargs={"k": 3}
    )

    # Reranker + Retriever ì••ì¶•ê¸° êµ¬ì„±
    model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
    compressor = CrossEncoderReranker(model=model, top_n=3)

    schema_retriever_compression = ContextualCompressionRetriever(
        base_compressor=compressor,
        base_retriever=schema_retriever
    )

    # Gemini LLM
    logging.info("ğŸ¤– Gemini LLM ì´ˆê¸°í™” ì¤‘...")
    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    GEMINI_API_BASE = os.environ.get("GEMINI_API_BASE")
    
    llm = GeminiSyncViaGMS(api_key=GEMINI_API_KEY, api_base=GEMINI_API_BASE)

    # ì²´ì¸ ì •ì˜
    rag_chain = (
        {
            "chat_history": RunnableLambda(lambda x: ""),  # í˜„ì¬ ëŒ€í™” ê¸°ë¡ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
            "context_schema": RunnableLambda(lambda x: schema_retriever_compression.invoke(x["question"])),
            "question": RunnableLambda(lambda x: x["question"]),
            "user_department": RunnableLambda(lambda x: x["user_department"])
        }
        | get_prompt(user_department)
        | llm
        | StrOutputParser()
    )
    
    inputs = {
        "question": question,
        "user_department": user_department,
    }

    answer = rag_chain.invoke(inputs)

    # print("\n[ì°¸ê³ ëœ ë¬¸ì„œ ì¶œì²˜]:")
    # for doc in rag_chain.last_run["source_documents"]:
    #     print("-", doc.metadata.get("source"))

    return answer

def run():
    from pinecone import Pinecone
    from langchain_pinecone import PineconeVectorStore
    from langchain.embeddings import HuggingFaceEmbeddings

    # í™˜ê²½ë³€ìˆ˜ì—ì„œ API ì •ë³´ ë¶ˆëŸ¬ì˜¤ê¸°
    PINECONE_API_KEY = os.environ["PINECONE_API_KEY"]
    PINECONE_ENV = os.environ.get("PINECONE_ENV", "us-east-1")  # ê¸°ë³¸ê°’ ì„¤ì •
    GEMINI_API_KEY = os.environ["GEMINI_API_KEY"]
    GEMINI_API_BASE = os.environ["GEMINI_API_BASE"]

    # Pinecone í´ë¼ì´ì–¸íŠ¸ ë° ì¸ë±ìŠ¤ ë¡œë“œ
    pc = Pinecone(api_key=PINECONE_API_KEY)
    index = pc.Index("schema-index")

    # HuggingFace ì„ë² ë”© ë¡œë“œ
    embedding = HuggingFaceEmbeddings(
        model_name="nlpai-lab/KURE-v1",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )

    # ê¸°ì¡´ ì¸ë±ìŠ¤ì— ì—°ê²°ëœ ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°
    schema_vectorstore = PineconeVectorStore(
        index=index,
        embedding=embedding,
        text_key="page_content"
    )

    # RAG ì²´ì¸ ì‹¤í–‰
    question = "ì„±ê³¼ê°€ ë¶€ì§„í•œ ë¶€ì„œì˜ ì„±ê³¼ê¸‰ì„ ì¡°ê¸ˆ ì¡°ì •í•´ì•¼í•  ê²ƒ ê°™ì•„. ì–¼ë§ˆ ì •ë„ê°€ ì ë‹¹í• ê¹Œ?"
    user_department = "ì¸ì‚¬íŒ€"

    answer = set_rag_chain(question, user_department, schema_vectorstore)

    print("\n[ìµœì¢… ì‘ë‹µ]")
    print(answer)

if __name__ == "__main__":
    run()
