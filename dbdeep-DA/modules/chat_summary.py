import logging
import re
import nltk
from typing import List, Dict
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s - %(message)s')

# nltk ì´ˆê¸°í™”
nltk.download('punkt')

# ëª¨ë¸ ë¡œë”©
logging.info("ðŸ”„ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained('eenzeenee/t5-base-korean-summarization') # 'eenzeenee/t5-base-korean-summarization', 'eenzeenee/t5-small-korean-summarization'
model = AutoModelForSeq2SeqLM.from_pretrained('eenzeenee/t5-base-korean-summarization') # 'eenzeenee/t5-base-korean-summarization', 'eenzeenee/t5-small-korean-summarization'
logging.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

def summarize_text(text: str) -> str:
    logging.info("ðŸ“ í…ìŠ¤íŠ¸ ìš”ì•½ ì‹œìž‘")
    input_text = "summarize: " + text.strip()
    logging.info("âœ‚ï¸ í† í°í™” ì¤‘...")
    inputs = tokenizer([input_text], max_length=512, truncation=True, return_tensors="pt")

    logging.info("ðŸ§  ìš”ì•½ ìƒì„± ì¤‘...")
    output = model.generate(**inputs, num_beams=3, do_sample=True, min_length=10, max_length=64)

    decoded = tokenizer.batch_decode(output, skip_special_tokens=True)[0].strip()

    if not decoded:
        logging.warning("âš ï¸ ëª¨ë¸ì´ ë¹ˆ ë¬¸ìžì—´ì„ ë°˜í™˜í•¨. ìš”ì•½ ì‹¤íŒ¨.")
        return ""

    sentences = nltk.sent_tokenize(decoded)
    if not sentences:
        logging.warning(f"âš ï¸ ë¬¸ìž¥ ë¶„ë¦¬ ì‹¤íŒ¨. ì „ì²´ ë°˜í™˜: {decoded}")
        return decoded  # fallback: ì „ì²´ ë°˜í™˜

    summary = sentences[0]
    logging.info(f"ðŸ“Œ ìš”ì•½ ê²°ê³¼: {summary}")
    return summary



def summarize_conversation(text: str) -> str:
    """ë‹¨ì¼ ëŒ€í™” í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½"""
    return summarize_text(text)


def should_use_context(query: str) -> bool:
    """ë§¥ë½ ì˜ì¡´í˜• ì§ˆì˜ íŒë‹¨"""
    contextual_keywords = ["ê·¸ëŸ¼", "ë‹¤ìŒì—”", "ì´ëŸ´ ë•Œ", "ê·¸ëž˜ì„œ", "ê·¸ ì´í›„", "ì´ ê²½ìš°"]
    for kw in contextual_keywords:
        if query.strip().startswith(kw):
            logging.info("ðŸ” ë§¥ë½ ì˜ì¡´ ì§ˆì˜ë¡œ íŒë‹¨ë¨")
            return True
    logging.info("ðŸ†• ë…ë¦½ ì§ˆì˜ë¡œ íŒë‹¨ë¨")
    return False


def build_context_prompt(history: List[Dict]) -> str:
    """
    ìµœê·¼ ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ context í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    window = history[-3:]  # ë§ˆì§€ë§‰ 2~3í„´ ì‚¬ìš©
    raw_text = ""
    for turn in window:
        role = "ì‚¬ìš©ìž" if turn["role"] == "user" else "AI"
        raw_text += f"{turn['content']}\n"

    summary = summarize_text(raw_text)
    return f"[Context]\n{summary}\n"


def summarize_chat_history(history: List[Dict]) -> str:
    """
    ì „ì²´ ížˆìŠ¤í† ë¦¬ê°€ ê¸¸ì–´ì¡Œì„ ë•Œ ê°„ì†Œí™” ìš”ì•½
    """
    raw_text = ""
    for turn in history:
        role = "ì‚¬ìš©ìž" if turn["role"] == "user" else "AI"
        raw_text += f"{role}: {turn['content']}\n"

    return summarize_text(raw_text)

def build_additional_prompt_with_history(history: List[Dict]) -> str:
    """
    ëŒ€í™” ížˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë§¥ë½ ì—¬ë¶€ íŒë‹¨ í›„ context í¬í•¨ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        history (List[Dict]): role(user/assistant)ê³¼ contentë¥¼ í¬í•¨í•œ ëŒ€í™” ì´ë ¥ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        str: LLMì— ë„£ì„ ìµœì¢… í”„ë¡¬í”„íŠ¸ ë¬¸ìžì—´
    """
    if not history:
        logging.warning("âš ï¸ ížˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ ìžˆì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    current_query = history[-1]["content"]

    if should_use_context(current_query):
        context_prompt = build_context_prompt(history[:-1])  # ë§ˆì§€ë§‰ ë°œí™” ì œì™¸í•˜ê³  ë§¥ë½ êµ¬ì„±
        final_prompt = f"[Current User Query]\n{current_query}\n{context_prompt}"
    else:
        final_prompt = current_query

    logging.info("ðŸ§¾ ìµœì¢… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ")
    return final_prompt


def summarize_history_if_needed(history: str, TOKEN_LIMIT: int = 3000) -> str:
    token_count = len(tokenizer.encode(history))
    if token_count < TOKEN_LIMIT:
        return history  # ìš”ì•½ í•„ìš” ì—†ìŒ
    else:
        return summarize_chat_history(history)  # ìš”ì•½ ëª¨ë¸ í˜¸ì¶œ

# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    logging.info("ðŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œìž‘")

    # ë‹¨ìˆœ ëŒ€í™” ìš”ì•½ í…ŒìŠ¤íŠ¸
    sample = """
        ì•ˆë…•í•˜ì„¸ìš”? ìš°ë¦¬ (2í•™ë…„)/(ì´ í•™ë…„) ì¹œêµ¬ë“¤ ìš°ë¦¬ ì¹œêµ¬ë“¤ í•™êµì— ê°€ì„œ ì§„ì§œ (2í•™ë…„)/(ì´ í•™ë…„) ì´ ë˜ê³  ì‹¶ì—ˆëŠ”ë° í•™êµì— ëª» ê°€ê³  ìžˆì–´ì„œ ë‹µë‹µí•˜ì£ ? 
        ê·¸ëž˜ë„ ìš°ë¦¬ ì¹œêµ¬ë“¤ì˜ ì•ˆì „ê³¼ ê±´ê°•ì´ ìµœìš°ì„ ì´ë‹ˆê¹Œìš” ì˜¤ëŠ˜ë¶€í„° ì„ ìƒë‹˜ì´ëž‘ ë§¤ì¼ ë§¤ì¼ êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ë³´ë„ë¡ í•´ìš”. 
        ì–´/ ì‹œê°„ì´ ë²Œì¨ ì´ë ‡ê²Œ ëë‚˜ìš”? ëŠ¦ì—ˆì–´ìš”. ëŠ¦ì—ˆì–´ìš”. ë¹¨ë¦¬ êµ­ì–´ ì—¬í–‰ì„ ë– ë‚˜ì•¼ ë¼ìš”. 
        ê·¸ëŸ°ë° ì–´/ êµ­ì–´ì—¬í–‰ì„ ë– ë‚˜ê¸° ì „ì— ìš°ë¦¬ê°€ ì¤€ë¹„ë¬¼ì„ ì±™ê²¨ì•¼ ë˜ê² ì£ ? êµ­ì–´ ì—¬í–‰ì„ ë– ë‚  ì¤€ë¹„ë¬¼, êµì•ˆì„ ì–´ë–»ê²Œ ë°›ì„ ìˆ˜ ìžˆëŠ”ì§€ ì„ ìƒë‹˜ì´ ì„¤ëª…ì„ í•´ì¤„ê²Œìš”. 
        (EBS)/(ì´ë¹„ì—ìŠ¤) ì´ˆë“±ì„ ê²€ìƒ‰í•´ì„œ ë“¤ì–´ê°€ë©´ìš” ì²«í™”ë©´ì´ ì´ë ‡ê²Œ ë‚˜ì™€ìš”. 
        ìž/ ê·¸ëŸ¬ë©´ìš” ì—¬ê¸° (X)/(ì—‘ìŠ¤) ëˆŒëŸ¬ì£¼(ê³ ìš”)/(êµ¬ìš”). ì €ê¸° (ë™ê·¸ë¼ë¯¸)/(ë˜¥ê·¸ë¼ë¯¸) (EBS)/(ì´ë¹„ì—ìŠ¤) (2ì£¼)/(ì´ ì£¼) ë¼ì´ë¸ŒíŠ¹ê°•ì´ë¼ê³  ë˜ì–´ìžˆì£ ? 
        ê±°ê¸°ë¥¼ ë°”ë¡œ ê°€ê¸°ë¥¼ ëˆ„ë¦…ë‹ˆë‹¤. ìž/ (ëˆ„ë¥´ë©´ìš”)/(ëˆŒë¥´ë©´ìš”). ì–´ë–»ê²Œ ë˜ëƒ? b/ ë°‘ìœ¼ë¡œ ë‚´ë ¤ìš” ë‚´ë ¤ìš” ë‚´ë ¤ìš” ì­‰ ë‚´ë ¤ìš”. 
        ìš°ë¦¬ ëª‡ í•™ë…„ì´ì£ ? ì•„/ (2í•™ë…„)/(ì´ í•™ë…„) ì´ì£  (2í•™ë…„)/(ì´ í•™ë…„)ì˜ ë¬´ìŠ¨ ê³¼ëª©? êµ­ì–´. 
        ì´ë²ˆì£¼ëŠ” (1ì£¼)/(ì¼ ì£¼) ì°¨ë‹ˆê¹Œìš” ì—¬ê¸° êµì•ˆ. ë‹¤ìŒì£¼ëŠ” ì—¬ê¸°ì„œ ë‹¤ìš´ì„ ë°›ìœ¼ë©´ ë¼ìš”. 
        ì´ êµì•ˆì„ í´ë¦­ì„ í•˜ë©´, ì§œìž”/. ì´ë ‡ê²Œ êµìž¬ê°€ ë‚˜ì˜µë‹ˆë‹¤ .ì´ êµì•ˆì„ (ë‹¤ìš´)/(ë”°ìš´)ë°›ì•„ì„œ ìš°ë¦¬ êµ­ì–´ì—¬í–‰ì„ ë– ë‚  ìˆ˜ê°€ ìžˆì–´ìš”. 
        ê·¸ëŸ¼ ìš°ë¦¬ ì§„ì§œë¡œ êµ­ì–´ ì—¬í–‰ì„ í•œë²ˆ ë– ë‚˜ë³´ë„ë¡ í•´ìš”? êµ­ì–´ì—¬í–‰ ì¶œë°œ. ìž/ (1ë‹¨ì›)/(ì¼ ë‹¨ì›) ì œëª©ì´ ë­”ê°€ìš”? í•œë²ˆ ì°¾ì•„ë´ìš”. 
        ì‹œë¥¼ ì¦ê²¨ìš” ì—ìš”. ê·¸ëƒ¥ ì‹œë¥¼ ì½ì–´ìš” ê°€ ì•„ë‹ˆì—ìš”. ì‹œë¥¼ ì¦ê²¨ì•¼ ë¼ìš” ì¦ê²¨ì•¼ ë¼. ì–´ë–»ê²Œ ì¦ê¸¸ê¹Œ? ì¼ë‹¨ì€ ë‚´ë‚´ ì‹œë¥¼ ì¦ê¸°ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ê³µë¶€ë¥¼ í•  ê±´ë°ìš”. 
        ê·¸ëŸ¼ ì˜¤ëŠ˜ì€ìš” ì–´ë–»ê²Œ ì¦ê¸¸ê¹Œìš”? ì˜¤ëŠ˜ ê³µë¶€í•  ë‚´ìš©ì€ìš” ì‹œë¥¼ ì—¬ëŸ¬ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì½ê¸°ë¥¼ ê³µë¶€í• ê²ë‹ˆë‹¤. 
        ì–´ë–»ê²Œ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ì½ì„ê¹Œ ìš°ë¦¬ ê³µë¶€í•´ ë³´ë„ë¡ í•´ìš”. ì˜¤ëŠ˜ì˜ ì‹œ ë‚˜ì™€ë¼ ì§œìž”/! ì‹œê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤ ì‹œì˜ ì œëª©ì´ ë­”ê°€ìš”? ë‹¤íˆ° ë‚ ì´ì—ìš” ë‹¤íˆ° ë‚ . 
        ëˆ„êµ¬ëž‘ ë‹¤í‰œë‚˜ ë™ìƒì´ëž‘ ë‹¤í‰œë‚˜ ì–¸ë‹ˆëž‘ ì¹œêµ¬ëž‘? ëˆ„êµ¬ëž‘ ë‹¤í‰œëŠ”ì§€ ì„ ìƒë‹˜ì´ ì‹œë¥¼ ì½ì–´ ì¤„ í…Œë‹ˆê¹Œ í•œë²ˆ ìƒê°ì„ í•´ë³´ë„ë¡ í•´ìš”."""

    result = summarize_conversation(sample)
    print("âœ… ë‹¨ì¼ ëŒ€í™” ìš”ì•½ ê²°ê³¼:\n", result)
    

    # ížˆìŠ¤í† ë¦¬ ê¸°ë°˜ context í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
    history = [
        {"role": "user", "content": "ì˜¤ëŠ˜ ìˆ˜ì—…ì€ ì–´ë–¤ ë‚´ìš©ì´ì—ˆì–´?"},
        {"role": "assistant", "content": "ì‹œë¥¼ ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ê°ìƒí•˜ëŠ” ë°©ë²•ì„ ë°°ì› ì–´ìš”."},
        {"role": "user", "content": "ê·¸ëŸ¼ ë‚´ì¼ ìˆ˜ì—…ì€ ë­ì•¼?"}
    ]
    
    final_prompt = build_additional_prompt_with_history(history)

    print("\nðŸ“¤ ìµœì¢… ìƒì„± í”„ë¡¬í”„íŠ¸:\n", final_prompt)
